<!DOCTYPE html>





<html lang="en">

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>
  
    
    The Datatrust
  
 | Computable Spec</title>



<link rel="stylesheet" href="../../book.min.f4161f5e2de53a2e927f51df1611323a2a12cccb2681f23cb6fc3517852e8643.css">


<link rel="icon" href="../../favicon.png" type="image/x-icon">


<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

  
</head>

<body>
  <input type="checkbox" style="display: none" id="menu-control" />
  <main class="flex container">

    <aside class="book-menu fixed">
      <nav>
<h2 class="book-brand">
  <a href="https://computablelabs.github.io/compspec/">Computable Spec</a>
</h2>



    
  
  
  

  <style>
  nav ul a[href$="\2f docs\2f datatrust\2f "] {
      color: #0b3a53;
  }
  </style>

<ul>
<li><a href="../../"><strong>Introduction</strong></a></li>
<li><a href="../../docs/contracts/">Contracts</a></li>
<li><a href="../../docs/markettoken/">Market Token</a></li>
<li><a href="../../docs/voting/">Voting</a></li>
<li><a href="../../docs/listings/">Listings</a></li>
<li><a href="../../docs/reserve/">Reserve</a></li>
<li><a href="../../docs/datatrust/">Datatrusts</a></li>
<li><a href="../../docs/parameters/">Market Parameters</a></li>
<li><a href="../../docs/attacks/">Attacks</a></li>
<li><a href="../../docs/userjourney/">User Journeys</a></li>
</ul>





</nav>


  
<script>
(function() {
  var menu = document.querySelector('aside.book-menu nav')
  addEventListener('beforeunload', function(event) {
    localStorage.setItem('menu.scrollTop', menu.scrollTop)
  });
  menu.scrollTop = localStorage.getItem('menu.scrollTop')
})()
</script>



    </aside>

    <div class="book-page">
      <header class="align-center justify-between book-header">
  <label for="menu-control">
    <img src="../../svg/menu.svg" alt="Menu" />
  </label>
  <strong>
  
    
    The Datatrust
  
</strong>
</header>

      
<article class="markdown">

<h2 id="the-datatrust">The Datatrust</h2>

<p>The off-chain portion of the Computable protocol, the
Datatrust, is responsible for storing, querying and
delivering upon data.</p>

<pre><code>    - [Buying Data](#paying-for-computation): Each `Market` supports running computational workloads against the data in this market. Workloads are run on a `Datatrust` tied to the market and may include SQL queries and standard programs capable of executing in a standard Linux environment. Users have to pay for workloads before they may execute them on a `Datatrust`.
        - [Data utilization](#data-utilization): The market maintains track of how many times each listing has been requested by different queries.
    - [Datatrusts](#authorized-backends): The data listed in the data market is held off-chain in a `Datatrust`. A council vote is used to set authorized backend systems for this market.
</code></pre>

<h3 id="datatrust-specification">Datatrust Specification</h3>

<p>A Datatrust is a system that is responsible for storing
data off-chain. As a first approximation, think of a
Datatrust as storing an off-chain key-value store that
maps <code>listingHash</code> to the actual listing data.</p>

<p>At present, a data market has only one datatrust. The
datatrust is currently a trusted entity in the
protocol. However, active research is under way to
reduce the trust requirements upon the datatrust.</p>

<h4 id="storage">Storage</h4>

<p>The Datatrust is responsible for storing the off-chain
data associated with listings. Listing candidates must
convey their off-chain candidate data to the Datatrust
node in order to be listed.</p>

<h4 id="computational-workloads">Computational Workloads</h4>

<p>The Computable protocol allows users to run workloads
on off-chain data. Payments for these workloads must be
made on-chain before workloads will be allowed to run.
In the current version of the protocol, security and
privacy guarantees are not enforced upon workloads, but
it is expected that future protocol iterations will
enforce such guarantees.</p>

<p>Users can expect that their workloads will run within a
fairly standard computing environment (some sandboxed
Linux environment likely) and that the raw data will be
exposed to the sandbox. Users will be able to run
standard SQL queries, and will also be able to use
standard Python machine learning tools to train models
and implement ETL pipelines. (Note that since the
sandbox will be some standard Linux, users are free to
use alternative pipelines).</p>

<p>The specification does not constrain <code>Backend</code>
implementers on their design choices, but to first
approximation, this feature should likely be
implemented by having new sandboxed compute nodes being
spun up dynamically to handle inbound queries. This
might be implemented for example by running the
computation within a docker container on a new EC2
node.</p>

<p>The user sends a string to the <code>Backend</code> holding the
script to be executed via the REST API. This will
usually be a bash script of some form coordinating the
execution of a program in the <code>Backend</code> Linux
environment. For ease, SQL might be handled in a
special case so the user doesn&rsquo;t need to set up a
suitable SQL environment via script each time.</p>

<p>As a note about implementation, a first design for the
workload support would be to have the <code>Backend</code> spin up
a preconfigured docker instance for each new user
script. The script would execute within this container.
Since container filesystems are ephemeral, scripts
wouldn&rsquo;t be capable of altering stored data within the
<code>Backend</code>.  Optionally, the container&rsquo;s network access
could be turned off so the workload can&rsquo;t directly
download <code>Backend</code> data.</p>

<p>Note furthermore that compute workloads may draw upon
data from multiple data markets. (The limitation of
course is that the <code>Backend</code> that the compute is
running on must be authorized for both data markets).</p>

<h4 id="coarse-data-utilization">Coarse Data Utilization</h4>

<p>The market is responsible for maintaining a record of
which listings have been accessed by which queries.
Doing this robustly is still an open <a href="#fine-grained-data-utilization">research
problem</a>. However, the
current <code>Backend</code> spec supports a coarse-grained record
by simply tracking the number of computations which
have been run on this data market. It&rsquo;s assumed that
each computation touched all listings. (This is
obviously wrong, but is useful for initial
implementation efforts).</p>

<pre><code>function update_listings_accessed(uint num_workloads) external
</code></pre>

<p>This function can only be called by an authorized <code>Backend</code>. At present, only reports the number of queries run on this <code>Backend</code>.</p>

<h4 id="rest-api">REST API</h4>

<p>The <code>Backend</code> is responsible for serving a number of endpoints. These endpoints are specified below. The syntax <code>Backend::ENDPOINT(input)</code> is used to specify that the <code>Backend</code> supports an <code>ENDPOINt</code> which expects <code>input</code>.</p>

<ul>
<li><code>Backend::GET_SCHEMA(market)</code>: Returns the schema for the given market.</li>
<li><code>Backend::RUN_QUERY(auth_token, markets, query)</code>: Runs the given query against the given markets. The query is sent in a query file.</li>
<li><code>Backend::ADD_DATAPOINT(auth_token, market, data)</code>: Adds the given data point to the <code>Backend</code> along with necessary authorization.</li>
<li><code>Backend::REMOVE_DATAPOINT(auth_token, market, data)</code>: Removes the given data point from the <code>Backend</code>.</li>
<li><code>Backend::GET_DATAPOINT(auth_token, market, dataHash)</code>: Fetches the data point uniquely associated with <code>dataHash</code> from the <code>Backend</code>.</li>
<li><code>Backend::MARKETS_SUPPORTED()</code>: Returns the list of <code>Market</code>s which <code>Backend</code> supports.</li>
<li><code>Backend::GET_COMPUTE_COST(query)</code>: A call to the <code>Backend</code> via REST to get the cost for running specified query.</li>
<li><code>Backend::GET_EPSILON(query)</code>: A call to the <code>Backend</code> via REST to get the epsilon privacy loss for running specified query.</li>
</ul>

<p>We&rsquo;ve represented these REST queries as methods, but remember that
these &ldquo;methods&rdquo; are actually REST API calls. The <code>auth_token</code> will be
contained in the request header and the other arguments will be passed
in the request body.</p>

<p>Let&rsquo;s dive into these methods in more detail.</p>

<h4 id="get-schema">GET_SCHEMA</h4>

<p><code>Backend::GET_SCHEMA(market)</code> returns the schema associated with a
particular data market. Note that the schema is associated closely
with an individual market. In a future iteration of the design, the
schema may be an on-chain entity.</p>

<p>Note that the results of <code>Backend::GET_SCHEMA(market)</code> should specify
the bytestring format that is accepted by <code>Backend::ADD_DATAPOINT</code>.
The <code>Backend</code> is responsible for rejecting malformed bytestrings that
don&rsquo;t adhere to the schema.</p>

<h4 id="get-datapoint">GET_DATAPOINT</h4>

<p>The call <code>Backend::GET_DATAPOINT(auth_token, market,
dataHash)</code> fetches the data point uniquely associated
with <code>dataHash</code> from the backend. It&rsquo;s worth pausing a
bit here to unpack this statement.  Recall that a
datapoint is a bytestring of arbitrary length. Why is
<code>dataHash</code> guaranteed to be uniquely identified with
this given bytestring? Recall that the hash function we
use is a cryptographic hash function. This means that
it&rsquo;s really hard to find collisions in cryptographic
hash functions (effectively impossible for a good
choice of cryptographic hash). So if the API call
returns a particular bytestring, the caller can
recompute the cryptographic hash locally and verify
that the hash of the returned value equals <code>dataHash</code>.</p>

<p>Let&rsquo;s take a brief digression and chat about
implementations. Although <code>dataHash</code> uniquely specifies
a datapoint, the <code>Backend</code> implementation must have an
implemented method to retrieve a given datapoint given
<code>dataHash</code>. The simplest possible implementation would
be for the <code>Backend</code> to store a flat file on disk of
form</p>

<pre><code>[bytestring_1,...,bytestring_n]
</code></pre>

<p>Now let&rsquo;s suppose <code>H</code> is our cryptographic hash
function. Given <code>dataHash</code>, the backend could retrieve
the associated bytestring with asimple for-loop</p>

<pre><code>for bytestring in [bytestring_1,...,bytestring_n]:
  if H(bytestring) == dataHash:
    return bytestring
</code></pre>

<p>This is very inefficient since it requires a full
<code>O(n)</code> traversal for every retrieval. Alternatively, we
could create a SQL table that stores this mapping for
us</p>

<pre><code>hash, data
H(bytestring_1), bytestring_1
.
.
.
H(bytestring_n), bytestring_n
</code></pre>

<p>Then if we denote <code>hash</code> as the primary key for this
table, we can use a standard SQL lookup command to
retrieve the correct bytestring associated with
<code>dataHash</code>.</p>

<h2 id="forwarding-looking-research">Forwarding Looking Research</h2>

<p>Features in this section are being actively researched
by the Computable team, but at present are not on the
roadmap for a particular Computable release. This is
expected to change as development matures, and it is
expected that all work in this section will eventually
migrate up into the concrete specifications part of
this document.</p>

<h3 id="fine-grained-data-utilization">Fine Grained Data Utilization</h3>

<p>Tracking fine-grained data usage is necessary for fair
distribution of user rewards. It&rsquo;s expected that some
listings in a <code>Market</code> will be significantly more
valuable than others. These listings should receive
greater rewards than less valuable listings. To first
approximation, we can track data usage by keeping track
of which listings a particular computation touches on
the <code>Backend</code> side. The <code>Backend</code> could then report
these records to the on-chain contracts, which would
then update the usage records.</p>

<p>This gets tricky for more complex workloads for
example. A deep learning model will train on all data,
but some listings will contribute more to the value of
the model than others. Robustly attributing importance
to particular data points over others is still an open
question in machine learning with only a few research
prototypes out there.</p>
</article>

      
<div class="book-footer justify-between">
  
  <div>
    
    <a href="https://github.com/computablelabs/compspec/commit/25700744bb32fcf9b41e5bb686ef13fbaef7e77e" title='Last modified Aug 22, 2019 by Bharath Ramsundar' target="_blank" rel="noopener">
      <img src="../../svg/calendar.svg" alt="Changed" /> Aug 22, 2019
    </a>
  </div>
  
  
  <div>
    <a href="https://github.com/computablelabs/compspec/edit/master/docs/datatrust.md" target="_blank" rel="noopener">
      <img src="../../svg/edit.svg" alt="Edit" /> Edit this page
    </a>
  </div>
  
</div>


      
    </div>

    
  

  <aside class="book-toc level-6 fixed">
    <nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#the-datatrust">The Datatrust</a>
<ul>
<li><a href="#datatrust-specification">Datatrust Specification</a>
<ul>
<li><a href="#storage">Storage</a></li>
<li><a href="#computational-workloads">Computational Workloads</a></li>
<li><a href="#coarse-data-utilization">Coarse Data Utilization</a></li>
<li><a href="#rest-api">REST API</a></li>
<li><a href="#get-schema">GET_SCHEMA</a></li>
<li><a href="#get-datapoint">GET_DATAPOINT</a></li>
</ul></li>
</ul></li>
<li><a href="#forwarding-looking-research">Forwarding Looking Research</a>
<ul>
<li><a href="#fine-grained-data-utilization">Fine Grained Data Utilization</a></li>
</ul></li>
</ul></li>
</ul>
</nav>
  </aside>



  </main>
  
  
  
</body>

</html>
